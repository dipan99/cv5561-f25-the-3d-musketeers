import os
import sys
import argparse
import torch
import numpy as np
from pathlib import Path
from time import time
import shutil
from types import SimpleNamespace
import PIL
import PIL.Image
from PIL.ImageOps import exif_transpose
from plyfile import PlyData, PlyElement
import matplotlib.pyplot as plt
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend

# Add VGGT directory to Python path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'vggt'))

os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'
from icecream import ic
ic(torch.cuda.is_available())

# Now import VGGT components with corrected paths
from vggt.models.vggt import VGGT
from vggt.utils.load_fn import load_and_preprocess_images_square
from vggt.utils.pose_enc import pose_encoding_to_extri_intri
from vggt.utils.geometry import unproject_depth_map_to_point_map
from vggt.utils.helper import create_pixel_coordinate_grid, randomly_limit_trues
from vggt.dependency.track_predict import predict_tracks
from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap, batch_np_matrix_to_pycolmap_wo_track

# Import MASt3R utilities
from utils.sfm_utils import (save_intrinsics, save_extrinsic, save_time, 
                             init_filestructure, get_sorted_image_files, split_train_test,
                             save_intrinsics_from_mats)
from utils.camera_utils import generate_interpolated_path
import pycolmap
import trimesh
import torch.nn.functional as F


def run_VGGT_with_confidence(model, images, dtype, device, resolution=518):
    """Run VGGT inference with full confidence extraction"""
    assert len(images.shape) == 4
    assert images.shape[1] == 3

    # Resize to VGGT's fixed resolution
    images = F.interpolate(images, size=(resolution, resolution), mode="bilinear", align_corners=False)

    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=dtype):
            images = images[None]  # add batch dimension
            
            # Get full predictions including all confidence types
            predictions = model(images)

    # Extract all available confidence types
    confidence_data = {}
    
    # 1. Depth confidence (always available)
    if "depth_conf" in predictions:
        confidence_data["depth_conf"] = predictions["depth_conf"].squeeze(0).cpu().numpy()
    
    # 2. World points confidence (if available)
    if "world_points_conf" in predictions:
        confidence_data["world_points_conf"] = predictions["world_points_conf"].squeeze(0).cpu().numpy()
    
    # 3. Extract other predictions
    extrinsic = predictions["pose_enc"].squeeze(0).cpu().numpy()
    depth_map = predictions["depth"].squeeze(0).cpu().numpy()
    
    # Convert pose encoding to camera parameters
    from vggt.utils.pose_enc import pose_encoding_to_extri_intri
    extrinsic_mat, intrinsic_mat = pose_encoding_to_extri_intri(
        predictions["pose_enc"], images.shape[-2:]
    )
    extrinsic_mat = extrinsic_mat.squeeze(0).cpu().numpy()
    intrinsic_mat = intrinsic_mat.squeeze(0).cpu().numpy()
    
    return extrinsic_mat, intrinsic_mat, depth_map, confidence_data, predictions


def save_comprehensive_confidence_data(model_path, confidence_data, points_3d, 
                                     pred_confs=None, pred_vis_scores=None, use_ba=False, sparse_0_path=None):
    """Save all available confidence data"""
    print("Saving comprehensive confidence data...")
    conf_dir = os.path.join(model_path, "confidence")
    os.makedirs(conf_dir, exist_ok=True)
    
    # Save depth confidence maps
    if "depth_conf" in confidence_data:
        depth_conf = confidence_data["depth_conf"]
        for i, conf_map in enumerate(depth_conf):
            np.save(os.path.join(conf_dir, f"depth_conf_{i:04d}.npy"), conf_map)
        
        # Save depth confidence statistics
        np.savez(os.path.join(conf_dir, "depth_conf_stats.npz"),
                mean=np.mean(depth_conf),
                std=np.std(depth_conf),
                min=np.min(depth_conf),
                max=np.max(depth_conf))
    
    # Save world points confidence (if available)
    if "world_points_conf" in confidence_data:
        print("--------------------------------")
        print("--------------------------------")
        print("--------------------------------")
        print("--------------------------------")
        print("IMPORTANT: SEEING THIS MEANS YOU ARE USING THE WORLD POINTS CONF GENERATED BY VGGT")
        print("Saving world points confidence generated by VGGT...")
        world_points_conf = confidence_data["world_points_conf"]
        # for i, conf_map in enumerate(world_points_conf):
        #     np.save(os.path.join(conf_dir, f"world_points_conf_{i:04d}.npy"), conf_map)
        
        # Save world points confidence statistics
        # np.savez(os.path.join(conf_dir, "world_points_conf_stats.npz"),
        #         mean=np.mean(world_points_conf),
        #         std=np.std(world_points_conf),
        #         min=np.min(world_points_conf),
        #         max=np.max(world_points_conf))
    
    # Save track confidence (if available)
    if use_ba and pred_confs is not None:
        np.save(os.path.join(conf_dir, "track_confidence.npy"), pred_confs)
        np.save(os.path.join(conf_dir, "visibility_scores.npy"), pred_vis_scores)
        
        np.savez(os.path.join(conf_dir, "track_conf_stats.npz"),
                mean=np.mean(pred_confs),
                std=np.std(pred_confs),
                min=np.min(pred_confs),
                max=np.max(pred_confs))
    
    # Determine best confidence for 3D points
    if "world_points_conf" in confidence_data:
        point_conf = confidence_data["world_points_conf"]
        conf_source = "world_points"
    elif "depth_conf" in confidence_data:
        point_conf = confidence_data["depth_conf"]
        conf_source = "depth"
    else:
        point_conf = np.ones_like(points_3d[..., 0])
        conf_source = "uniform"
    
    # Save point-level confidence in sparse/0 directory
    if sparse_0_path is not None:
        np.save(os.path.join(sparse_0_path, "confidence.npy"), point_conf)
        print(f"Point3D confidence saved to {os.path.join(sparse_0_path, 'confidence.npy')}")
    
    # Also save in confidence directory for backup
    np.save(os.path.join(conf_dir, "point3d_confidence.npy"), point_conf)
    
    # Save metadata
    metadata = {
        'confidence_source': conf_source,
        'use_ba': use_ba,
        'available_confidence_types': list(confidence_data.keys())
    }
    np.savez(os.path.join(conf_dir, "confidence_metadata.npz"), **metadata)
    
    print(f"Confidence data saved to {conf_dir}")
    print(f"Primary confidence source: {conf_source}")
    
    return point_conf, conf_source


def save_processed_images(sparse_0_path, images_tensor, image_files, image_suffix, prefix="resized_"):
    """Save the processed 512x512 images used by VGGT"""
    print("Saving processed images...")
    
    # Create directory for processed images
    processed_img_dir = os.path.join(os.path.dirname(sparse_0_path), "processed_images")
    os.makedirs(processed_img_dir, exist_ok=True)
    
    # Convert tensor back to PIL images and save
    for i, img_tensor in enumerate(images_tensor):
        # Convert from tensor [3, H, W] to numpy [H, W, 3]
        img_np = img_tensor.cpu().numpy().transpose(1, 2, 0)
        img_np = (img_np * 255).astype(np.uint8)
        
        # Convert to PIL Image
        img_pil = PIL.Image.fromarray(img_np)
        
        # Get original filename
        original_name = os.path.basename(image_files[i])
        name_without_ext = os.path.splitext(original_name)[0]
        
        # Save with prefix
        save_path = os.path.join(processed_img_dir, f"{prefix}{name_without_ext}.{image_suffix}")
        img_pil.save(save_path)
        
        print(f"Saved processed image: {save_path}")
    
    print(f"All processed images saved to: {processed_img_dir}")
    return processed_img_dir


def apply_confidence_based_filtering(points_3d, conf_values, points_rgb, points_xyf, max_points=100000, output_dir=None):
    """
    Smart filtering based on confidence: sort by confidence and select top max_points points
    Includes detailed confidence statistics and visualization
    """
    print(f">> Applying confidence-based filtering...")
    print(f"Original points: {points_3d.shape}")
    print(f"Original confidence: {conf_values.shape}")
    
    # Flatten all arrays to 1D
    points_flat = points_3d.reshape(-1, 3)  # (N, 3)
    conf_flat = conf_values.reshape(-1)     # (N,)
    rgb_flat = points_rgb.reshape(-1, 3)    # (N, 3)
    xyf_flat = points_xyf.reshape(-1, 3)    # (N, 3)
    
    n_total_points = len(points_flat)
    print(f"Total flattened points: {n_total_points}")
    
    # Ensure all arrays have the same length
    assert len(conf_flat) == n_total_points, f"Confidence shape mismatch: {len(conf_flat)} vs {n_total_points}"
    assert len(rgb_flat) == n_total_points, f"RGB shape mismatch: {len(rgb_flat)} vs {n_total_points}"
    assert len(xyf_flat) == n_total_points, f"XYF shape mismatch: {len(xyf_flat)} vs {n_total_points}"
    
    # ======================== Detailed confidence statistics analysis ========================
    print("\n" + "="*60)
    print("CONFIDENCE STATISTICS ANALYSIS")
    print("="*60)
    
    # Basic statistics
    conf_min = np.min(conf_flat)
    conf_max = np.max(conf_flat)
    conf_mean = np.mean(conf_flat)
    conf_median = np.median(conf_flat)
    conf_std = np.std(conf_flat)
    
    # Quartiles
    conf_q1 = np.percentile(conf_flat, 25)    # 1st quartile (25%)
    conf_q3 = np.percentile(conf_flat, 75)    # 3rd quartile (75%)
    conf_iqr = conf_q3 - conf_q1              # Interquartile range
    
    # Other percentiles
    conf_p5 = np.percentile(conf_flat, 5)     # 5%
    conf_p95 = np.percentile(conf_flat, 95)   # 95%
    conf_p99 = np.percentile(conf_flat, 99)   # 99%
    
    print(f"Min:        {conf_min:.6f}")
    print(f"Q1 (25%):   {conf_q1:.6f}")
    print(f"Median:     {conf_median:.6f}")
    print(f"Mean:       {conf_mean:.6f}")
    print(f"Q3 (75%):   {conf_q3:.6f}")
    print(f"Max:        {conf_max:.6f}")
    print(f"Std:        {conf_std:.6f}")
    print(f"IQR:        {conf_iqr:.6f}")
    print(f"5th percentile:   {conf_p5:.6f}")
    print(f"95th percentile:  {conf_p95:.6f}")
    print(f"99th percentile:  {conf_p99:.6f}")
    
    # Statistics
    high_conf_count = np.sum(conf_flat > conf_q3)
    med_conf_count = np.sum((conf_flat >= conf_q1) & (conf_flat <= conf_q3))
    low_conf_count = np.sum(conf_flat < conf_q1)
    
    print(f"\nConfidence Distribution:")
    print(f"High confidence (>Q3): {high_conf_count:,} points ({100*high_conf_count/n_total_points:.1f}%)")
    print(f"Medium confidence (Q1-Q3): {med_conf_count:,} points ({100*med_conf_count/n_total_points:.1f}%)")
    print(f"Low confidence (<Q1): {low_conf_count:,} points ({100*low_conf_count/n_total_points:.1f}%)")
    
    # Sort by confidence (highest confidence first)
    sorted_indices = np.argsort(conf_flat)[::-1]
    sorted_conf = conf_flat[sorted_indices]
    
    # Select top max_points highest confidence points
    if n_total_points > max_points:
        selected_indices = sorted_indices[:max_points]
        print(f"\nSelected top {max_points:,} points from {n_total_points:,} (keeping {100*max_points/n_total_points:.1f}%)")
        
        # Statistics of confidence of selected points
        selected_conf = sorted_conf[:max_points]
        selected_min = selected_conf.min()
        selected_max = selected_conf.max()
        selected_mean = selected_conf.mean()
        selected_median = np.median(selected_conf)
        
        print(f"Selected points confidence:")
        print(f"  Range: [{selected_min:.6f}, {selected_max:.6f}]")
        print(f"  Mean: {selected_mean:.6f}")
        print(f"  Median: {selected_median:.6f}")
        print(f"  Confidence threshold (lowest selected): {selected_min:.6f}")
        
    else:
        selected_indices = sorted_indices
        selected_conf = sorted_conf
        print(f"\nKeeping all {n_total_points:,} points (below max_points limit)")
    
    # Filter all arrays
    filtered_points = points_flat[selected_indices]
    filtered_conf = conf_flat[selected_indices]
    filtered_rgb = rgb_flat[selected_indices]
    filtered_xyf = xyf_flat[selected_indices]
    
    # ======================== Visualize confidence distribution ========================
    if output_dir is not None:
        print(f"\n>> Creating confidence visualization...")
        
        # Create figure
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('Confidence Analysis', fontsize=16, fontweight='bold')
        
        # 1. Sorted confidence curve
        x_all = np.arange(len(sorted_conf))
        ax1.plot(x_all, sorted_conf, 'b-', linewidth=1, alpha=0.7, label='All points')
        if len(selected_conf) < len(sorted_conf):
            ax1.plot(x_all[:len(selected_conf)], selected_conf, 'r-', linewidth=2, label=f'Selected top {len(selected_conf):,}')
            ax1.axvline(x=len(selected_conf), color='red', linestyle='--', alpha=0.8, label='Selection cutoff')
        
        ax1.set_xlabel('Point Index (sorted by confidence)')
        ax1.set_ylabel('Confidence Value')
        ax1.set_title('Sorted Confidence Values')
        ax1.grid(True, alpha=0.3)
        ax1.legend()
        
        # 2. Confidence histogram
        ax2.hist(conf_flat, bins=100, alpha=0.7, color='skyblue', edgecolor='black')
        ax2.axvline(conf_mean, color='red', linestyle='-', linewidth=2, label=f'Mean: {conf_mean:.4f}')
        ax2.axvline(conf_median, color='green', linestyle='-', linewidth=2, label=f'Median: {conf_median:.4f}')
        ax2.axvline(conf_q1, color='orange', linestyle='--', linewidth=1.5, label=f'Q1: {conf_q1:.4f}')
        ax2.axvline(conf_q3, color='orange', linestyle='--', linewidth=1.5, label=f'Q3: {conf_q3:.4f}')
        if len(selected_conf) < len(sorted_conf):
            ax2.axvline(selected_conf.min(), color='purple', linestyle='-', linewidth=2, 
                       label=f'Selection threshold: {selected_conf.min():.4f}')
        
        ax2.set_xlabel('Confidence Value')
        ax2.set_ylabel('Frequency')
        ax2.set_title('Confidence Distribution')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. Log-scale confidence curve (better show tail)
        ax3.semilogy(x_all, sorted_conf, 'b-', linewidth=1, alpha=0.7)
        if len(selected_conf) < len(sorted_conf):
            ax3.semilogy(x_all[:len(selected_conf)], selected_conf, 'r-', linewidth=2)
            ax3.axvline(x=len(selected_conf), color='red', linestyle='--', alpha=0.8)
        
        ax3.set_xlabel('Point Index (sorted by confidence)')
        ax3.set_ylabel('Confidence Value (log scale)')
        ax3.set_title('Sorted Confidence Values (Log Scale)')
        ax3.grid(True, alpha=0.3)
        
        # 4. Comparison of selected vs unselected points
        if len(selected_conf) < len(sorted_conf):
            rejected_conf = sorted_conf[len(selected_conf):]
            
            bins = np.linspace(conf_min, conf_max, 50)
            ax4.hist(selected_conf, bins=bins, alpha=0.7, color='green', label=f'Selected ({len(selected_conf):,})', density=True)
            ax4.hist(rejected_conf, bins=bins, alpha=0.7, color='red', label=f'Rejected ({len(rejected_conf):,})', density=True)
            ax4.set_xlabel('Confidence Value')
            ax4.set_ylabel('Density')
            ax4.set_title('Selected vs Rejected Points')
            ax4.legend()
            ax4.grid(True, alpha=0.3)
        else:
            ax4.text(0.5, 0.5, 'All points selected\n(below max_points limit)', 
                    ha='center', va='center', transform=ax4.transAxes, fontsize=14)
            ax4.set_title('Selection Status')
        
        plt.tight_layout()
        
        # Save figure
        conf_viz_path = os.path.join(output_dir, "confidence_analysis.png")
        plt.savefig(conf_viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"Confidence visualization saved to: {conf_viz_path}")
        
        # Save detailed statistics to file
        stats_path = os.path.join(output_dir, "confidence_statistics.txt")
        with open(stats_path, 'w') as f:
            f.write("CONFIDENCE STATISTICS ANALYSIS\n")
            f.write("="*60 + "\n\n")
            f.write(f"Total points: {n_total_points:,}\n")
            f.write(f"Selected points: {len(selected_conf):,}\n")
            f.write(f"Selection ratio: {100*len(selected_conf)/n_total_points:.2f}%\n\n")
            
            f.write("Basic Statistics:\n")
            f.write(f"  Min:        {conf_min:.6f}\n")
            f.write(f"  Q1 (25%):   {conf_q1:.6f}\n")
            f.write(f"  Median:     {conf_median:.6f}\n")
            f.write(f"  Mean:       {conf_mean:.6f}\n")
            f.write(f"  Q3 (75%):   {conf_q3:.6f}\n")
            f.write(f"  Max:        {conf_max:.6f}\n")
            f.write(f"  Std:        {conf_std:.6f}\n")
            f.write(f"  IQR:        {conf_iqr:.6f}\n\n")
            
            f.write("Percentiles:\n")
            f.write(f"  5th:        {conf_p5:.6f}\n")
            f.write(f"  95th:       {conf_p95:.6f}\n")
            f.write(f"  99th:       {conf_p99:.6f}\n\n")
            
            if len(selected_conf) < len(sorted_conf):
                f.write("Selected Points Statistics:\n")
                f.write(f"  Range: [{selected_conf.min():.6f}, {selected_conf.max():.6f}]\n")
                f.write(f"  Mean: {selected_conf.mean():.6f}\n")
                f.write(f"  Median: {np.median(selected_conf):.6f}\n")
                f.write(f"  Selection threshold: {selected_conf.min():.6f}\n")
        
        print(f"Detailed statistics saved to: {stats_path}")
    
    print("="*60)
    print(f"Final filtered points shape: {filtered_points.shape}")
    print(f"Final confidence range: [{filtered_conf.min():.4f}, {filtered_conf.max():.4f}]")
    print(f"Final confidence mean: {filtered_conf.mean():.4f}")
    
    return filtered_points, filtered_conf.reshape(-1, 1), filtered_rgb, filtered_xyf


def main(source_path, model_path, device, min_conf_thr, llffhold, n_views, 
         image_size=518, use_ba=False, infer_video=False, **vggt_kwargs):

    # Initialize MASt3R-style directory structure
    save_path, sparse_0_path, sparse_1_path = init_filestructure(Path(source_path), n_views)
    
    # Load VGGT model
    model = VGGT()
    local_model_path = "/root/autodl-tmp/tamu/instantsplat-2dgs-vggt-dirty/VGGT-1B/models--facebook--VGGT-1B/snapshots/860abec7937da0a4c03c41d3c269c366e82abdf9/model.pt"
    if os.path.exists(local_model_path):
        print(f"Loading local VGGT model from {local_model_path}")
        model.load_state_dict(torch.load(local_model_path, map_location=device))
    else:
        print("Local model not found, downloading from HuggingFace...")
        _URL = "https://huggingface.co/facebook/VGGT-1B/resolve/main/model.pt"
        model.load_state_dict(torch.hub.load_state_dict_from_url(_URL, map_location=device))
    
    model.eval()
    model = model.to(device)
    print(f"VGGT model loaded")

    # Load and process images
    image_dir = Path(source_path) / 'images'
    image_files, image_suffix = get_sorted_image_files(image_dir)
    
    if infer_video:
        train_img_files = image_files
    else:
        train_img_files, test_img_files = split_train_test(image_files, llffhold, n_views, verbose=True)
    
    image_files = train_img_files
    image_path_list = [str(f) for f in image_files]
    
    # VGGT processing parameters - Follow MASt3R pattern
    vggt_fixed_resolution = 518  # Required by VGGT model
    img_load_resolution = image_size
    
    # Load images at specified size (like MASt3R)
    images, original_coords = load_and_preprocess_images_square(image_path_list, img_load_resolution)
    images = images.to(device)
    original_coords = original_coords.to(device)
    
    print(f"Loaded images at resolution: {img_load_resolution}")
    
    # Run VGGT with full confidence extraction
    start_time = time()
    print(f'>> Running VGGT with confidence extraction...')
    
    dtype = torch.bfloat16 if torch.cuda.get_device_capability()[0] >= 8 else torch.float16
    extrinsic, intrinsic, depth_map, confidence_data, full_predictions = run_VGGT_with_confidence(
        model, images, dtype, device, vggt_fixed_resolution
    )
    
    # *** KEY FIX: FORCE UNIFORM FOCAL LENGTHS LIKE MAST3R ***
    print(f"[BEFORE] VGGT varying focal lengths:")
    for i in range(min(5, len(intrinsic))):
        print(f"  Camera {i}: fx={intrinsic[i,0,0]:.2f}, fy={intrinsic[i,1,1]:.2f}")
    
    # Use MASt3R's uniform focal length approach
    uniform_focal = 506.08306884765625  # Same as MASt3R
    print(f"[APPLYING] Uniform focal length: {uniform_focal}")
    
    # Override VGGT's varying focal lengths with uniform ones
    intrinsic[:, 0, 0] = uniform_focal  # fx
    intrinsic[:, 1, 1] = uniform_focal  # fy
    
    # Set principal points to image center (VGGT resolution)
    intrinsic[:, 0, 2] = vggt_fixed_resolution / 2.0  # cx = 259.0
    intrinsic[:, 1, 2] = vggt_fixed_resolution / 2.0  # cy = 259.0
    
    print(f"[AFTER] Uniform intrinsics at VGGT resolution:")
    print(f"  fx=fy={uniform_focal}, cx=cy={vggt_fixed_resolution/2.0}")
    
    # Unproject to 3D points
    points_3d = unproject_depth_map_to_point_map(depth_map, extrinsic, intrinsic)
    points_3d_unfiltered = points_3d.copy()  # Save original before any filtering
    np.save(os.path.join(sparse_0_path, "points3D_all.npy"), points_3d_unfiltered.reshape(-1, 3))
    print(f"Saved raw 3D points to {sparse_0_path}/points3D_all.npy with shape {points_3d_unfiltered.reshape(-1, 3).shape}")
    
    # Process based on mode
    if use_ba:
        # Bundle Adjustment mode with track confidence
        print(f'>> Running Bundle Adjustment mode...')
        image_size_array = np.array(images.shape[-2:])
        scale = img_load_resolution / vggt_fixed_resolution  # Scale from 518 back to image_size

        with torch.cuda.amp.autocast(dtype=dtype):
            pred_tracks, pred_vis_scores, pred_confs, points_3d, points_rgb = predict_tracks(
                images,
                conf=confidence_data.get("depth_conf", np.ones_like(depth_map)),
                points_3d=points_3d,
                masks=None,
                max_query_pts=vggt_kwargs.get('max_query_pts', 4096),
                query_frame_num=vggt_kwargs.get('query_frame_num', 8),
                keypoint_extractor="aliked+sp",
                fine_tracking=vggt_kwargs.get('fine_tracking', True),
            )

        # Rescale intrinsics
        intrinsic[:, :2, :] *= scale
        track_mask = pred_vis_scores > vggt_kwargs.get('vis_thresh', 0.2)

        # Extract just the filenames for COLMAP (WITH extension)
        image_names = [os.path.basename(f) for f in image_path_list]

        reconstruction, valid_track_mask = batch_np_matrix_to_pycolmap_with_names(
            points_3d, extrinsic, intrinsic, image_size_array,
            image_names=image_names,
            masks=track_mask, 
            max_reproj_error=vggt_kwargs.get('max_reproj_error', 8.0),
            shared_camera=vggt_kwargs.get('shared_camera', False), 
            camera_type=vggt_kwargs.get('camera_type', 'SIMPLE_PINHOLE'), 
            points_rgb=points_rgb,
        )

        if reconstruction is None:
            raise ValueError("No reconstruction can be built with BA")

        ba_options = pycolmap.BundleAdjustmentOptions()
        pycolmap.bundle_adjustment(reconstruction, ba_options)
        reconstruction_resolution = img_load_resolution
        
    else:
        # Feedforward mode
        print(f'>> Running feedforward mode...')
        
        # Use best available confidence
        if "world_points_conf" in confidence_data:
            conf_values = confidence_data["world_points_conf"]
            print("Using world_points_conf from VGGT")
        else:
            conf_values = confidence_data.get("depth_conf", np.ones_like(depth_map))
            print("Using depth_conf as fallback")
        
        max_points_for_colmap = 1000000
        
        # Compute original image size and scaling from VGGT resolution (518)
        train_img_sample = PIL.Image.open(train_img_files[0])
        train_img_sample = exif_transpose(train_img_sample)
        org_width, org_height = train_img_sample.size  # (W, H) after EXIF orientation
        scale_x_to_org = org_width / vggt_fixed_resolution
        scale_y_to_org = org_height / vggt_fixed_resolution
        # Use original image size for reconstruction
        image_size_array = np.array([org_height, org_width])

        num_frames, height, width, _ = points_3d.shape

        # Prepare RGB data
        points_rgb = F.interpolate(
            images, size=(vggt_fixed_resolution, vggt_fixed_resolution), mode="bilinear", align_corners=False
        )
        points_rgb = (points_rgb.cpu().numpy() * 255).astype(np.uint8)
        points_rgb = points_rgb.transpose(0, 2, 3, 1)

        # Prepare pixel coordinates and scale to original size
        points_xyf = create_pixel_coordinate_grid(num_frames, height, width)
        points_xyf[..., 0] *= scale_x_to_org
        points_xyf[..., 1] *= scale_y_to_org
        used_original_coords = False
        try:
            # Try to use original_coords if it provides per-pixel mapping back to original image
            oc = original_coords.detach().cpu().numpy()
            if len(oc.shape) == 4:
                if oc.shape[1] == 2:  # (N, 2, H, W) -> (N, H, W, 2)
                    oc_np = np.transpose(oc, (0, 2, 3, 1))
                elif oc.shape[-1] == 2:  # (N, H, W, 2)
                    oc_np = oc
                else:
                    oc_np = None
                if oc_np is not None and oc_np.shape[1] == height and oc_np.shape[2] == width:
                    oc_min, oc_max = float(oc_np.min()), float(oc_np.max())
                    # If likely normalized, scale to pixels
                    if oc_max <= 1.5:
                        oc_np[..., 0] *= org_width
                        oc_np[..., 1] *= org_height
                    # Build (x, y, f) with frame index
                    frame_idx = np.arange(num_frames, dtype=np.float32).reshape(num_frames, 1, 1, 1)
                    frame_idx = np.repeat(frame_idx, height, axis=1)
                    frame_idx = np.repeat(frame_idx, width, axis=2)
                    points_xyf = np.concatenate([oc_np, frame_idx], axis=-1)
                    used_original_coords = True
                    print(f"[Feedforward] Using original_coords mapping (min={oc_min:.4f}, max={oc_max:.4f}) for pixel coordinates")
        except Exception as _:
            pass
        print(f"[Feedforward] org_size: (W={org_width}, H={org_height}), scale_x={scale_x_to_org:.4f}, scale_y={scale_y_to_org:.4f}, use_original_coords={used_original_coords}")
        
        # **Key modification: use smart filtering based on confidence instead of random filtering**
        points_3d_filtered, filtered_conf_values, points_rgb_filtered, points_xyf_filtered = apply_confidence_based_filtering(
            points_3d, conf_values, points_rgb, points_xyf, 
            max_points=max_points_for_colmap,
            output_dir=model_path  # Pass output directory for visualization
        )
        
        # Update variables to filtered results
        points_3d = points_3d_filtered
        points_rgb = points_rgb_filtered
        points_xyf = points_xyf_filtered
        # filtered_conf_values already has correct shape (N, 1)

        # Extract just the filenames for COLMAP (WITH extension)
        image_names = [os.path.basename(f) for f in image_path_list]  # Keep full file names

        # *** CRITICAL FIX: CORRECT INTRINSIC SCALING TO ORIGINAL IMAGE SIZE ***
        intrinsic_rescaled = intrinsic.copy()
        
        print(f"[SCALING] Before scaling to original size:")
        print(f"  Original image size: {org_width}x{org_height}")
        print(f"  VGGT resolution: {vggt_fixed_resolution}x{vggt_fixed_resolution}")
        print(f"  Scale factors: scale_x={scale_x_to_org:.4f}, scale_y={scale_y_to_org:.4f}")
        print(f"  Intrinsic before scaling: fx={intrinsic[0,0,0]:.2f}, fy={intrinsic[0,1,1]:.2f}, cx={intrinsic[0,0,2]:.2f}, cy={intrinsic[0,1,2]:.2f}")
        
        # Apply correct scaling math
        # intrinsic_rescaled[:, 0, 0] *= scale_x_to_org  # fx (focal length scales)
        # intrinsic_rescaled[:, 1, 1] *= scale_y_to_org  # fy (focal length scales)
        intrinsic_rescaled[:, 0, 0] = 506.08306884765625  # fx
        intrinsic_rescaled[:, 1, 1] = 506.08306884765625  # fy
        # *** FIX THE PRINCIPAL POINT SCALING BUG ***
        # WRONG (causes huge values): intrinsic_rescaled[:, 0, 2] *= scale_x_to_org+ox
        # CORRECT:
        intrinsic_rescaled[:, 0, 2] = intrinsic_rescaled[:, 0, 2] * scale_x_to_org  # cx
        intrinsic_rescaled[:, 1, 2] = intrinsic_rescaled[:, 1, 2] * scale_y_to_org  # cy
        
        print(f"[SCALING] After correct scaling:")
        print(f"  Intrinsic after scaling: fx={intrinsic_rescaled[0,0,0]:.2f}, fy={intrinsic_rescaled[0,1,1]:.2f}, cx={intrinsic_rescaled[0,0,2]:.2f}, cy={intrinsic_rescaled[0,1,2]:.2f}")
        
        # Verify principal points are reasonable (should be near image center)
        expected_cx = org_width / 2.0
        expected_cy = org_height / 2.0
        actual_cx = intrinsic_rescaled[0, 0, 2]
        actual_cy = intrinsic_rescaled[0, 1, 2]
        
        print(f"[VALIDATION] Principal point check:")
        print(f"  Expected cx≈{expected_cx:.1f}, actual cx={actual_cx:.1f}")
        print(f"  Expected cy≈{expected_cy:.1f}, actual cy={actual_cy:.1f}")
        
        if abs(actual_cx - expected_cx) > 50 or abs(actual_cy - expected_cy) > 50:
            print(f"[WARNING] Principal points seem off, forcing to image center")
            intrinsic_rescaled[:, 0, 2] = expected_cx  # cx = width/2
            intrinsic_rescaled[:, 1, 2] = expected_cy  # cy = height/2

        reconstruction = batch_np_matrix_to_pycolmap_wo_track_with_names(
            points_3d, points_xyf, points_rgb, extrinsic, intrinsic_rescaled, image_size_array,
            image_names=image_names,
            shared_camera=False, camera_type="PINHOLE",
        )
        reconstruction_resolution = img_load_resolution
        pred_confs = None
        pred_vis_scores = None

    # Save comprehensive confidence data with filtered confidence
    if not use_ba:
        # Feedforward mode: use filtered confidence directly
        final_point_conf = filtered_conf_values  # Already shaped as (N, 1)
        conf_source = "world_points" if "world_points_conf" in confidence_data else "depth"
        
        print(f"Final feedforward confidence shape: {final_point_conf.shape}")
        print(f"Final points shape: {points_3d.shape}")
        assert final_point_conf.shape[0] == len(points_3d), f"Shape mismatch: conf {final_point_conf.shape[0]} vs points {len(points_3d)}"
        
    else:
        # BA mode: use pred_confs if available
        num_final_points = len(points_3d.reshape(-1, 3))
        if pred_confs is not None:
            final_point_conf = pred_confs.reshape(-1, 1)
            if final_point_conf.shape[0] != num_final_points:
                print(f"BA mode: Adjusting confidence shape from {final_point_conf.shape[0]} to {num_final_points}")
                final_point_conf = final_point_conf[:num_final_points] if final_point_conf.shape[0] > num_final_points else np.vstack([final_point_conf, np.ones((num_final_points - final_point_conf.shape[0], 1))])
            conf_source = "track_confidence"
        else:
            final_point_conf = np.ones((num_final_points, 1))
            conf_source = "uniform_ba"
        
        print(f"Final BA confidence shape: {final_point_conf.shape}")

    # Save confidence data
    point_conf, _ = save_comprehensive_confidence_data(
        model_path, confidence_data, points_3d, pred_confs, pred_vis_scores, use_ba, sparse_0_path
    )

    # Save final confidence array for training
    np.save(os.path.join(sparse_0_path, "confidence.npy"), final_point_conf)
    np.save(os.path.join(sparse_0_path, "confidence_dsp.npy"), final_point_conf)  # Backup
    print(f"Saved final confidence with shape {final_point_conf.shape} to {sparse_0_path}/confidence.npy")
    print(f"Confidence source: {conf_source}")
    
    # Save COLMAP reconstruction in binary format first
    reconstruction.write(str(sparse_0_path))
    
    # Convert binary to text format
    convert_colmap_bin_to_txt(sparse_0_path)

    # *** NEW SECTION: Generate cameras.txt using same method as init_test_pose_vggt.py ***
    print(f'>> Generating cameras.txt using direct method (same as test pose initialization)...')

    # Extract focal lengths from VGGT intrinsic matrices
    train_focals = []
    for i in range(len(train_img_files)):
        focal_x = intrinsic[i, 0, 0]
        focal_y = intrinsic[i, 1, 1]
        # Use average focal length per camera (same as test pose method)
        avg_focal = (focal_x + focal_y) / 2.0
        train_focals.append(avg_focal)

    train_focals = np.array(train_focals)

    # Get original image shapes for training images
    # Load a training image to get its original dimensions
    train_img_sample = PIL.Image.open(train_img_files[0])
    train_img_sample = exif_transpose(train_img_sample)
    train_img_org_shape = train_img_sample.size  # (width, height) after EXIF orientation
    train_imgs_shape = (len(train_img_files), img_load_resolution, img_load_resolution, 3)  # Processing shape

    # Prefer writing exact per-image intrinsics (already rescaled for reconstruction)
    try:
        org_sizes = []
        for f in train_img_files:
            im = exif_transpose(PIL.Image.open(f))
            org_sizes.append(im.size)
        save_intrinsics_from_mats(sparse_0_path, intrinsic_rescaled, org_sizes)
    except Exception as _:
        # Fallback to focal-scaling method from 518 to original size
        train_imgs_shape = (len(train_img_files), vggt_fixed_resolution, vggt_fixed_resolution, 3)
        save_intrinsics(sparse_0_path, train_focals, train_img_org_shape, train_imgs_shape, save_focals=True)

    print(f'Training cameras.txt generated using direct method')
    print(f'Training focal lengths: {train_focals}')
    print(f'Average training focal: {np.mean(train_focals):.2f}')

    # Save the non-scaled focal lengths for later use by test pose initialization
    np.save(os.path.join(sparse_0_path, 'non_scaled_focals.npy'), train_focals)
    print(f'Saved non-scaled focals to: {os.path.join(sparse_0_path, "non_scaled_focals.npy")}')

    # Remove POINTS2D data rows, keep only IMAGE_ID rows
    # keep_only_image_id_rows(sparse_0_path)
    
    # Extract 3D points from COLMAP reconstruction for PLY export
    colmap_points_3d = []
    colmap_colors = []
    colmap_point_ids = []
    
    for point_id, point in reconstruction.points3D.items():
        colmap_points_3d.append(point.xyz)
        colmap_colors.append(point.color)
        colmap_point_ids.append(point_id)
    
    if len(colmap_points_3d) > 0:
        colmap_points_3d = np.array(colmap_points_3d)
        colmap_colors = np.array(colmap_colors)
        
        # Save with proper format for dataset_readers.py
        save_points3d_ply_with_normals(sparse_0_path, colmap_points_3d, colmap_colors)
    
    # Save confidence-enhanced point clouds
    # Use the final_point_conf we created above
    try:
        point_conf_flat = final_point_conf
    except NameError:
        # Fallback if final_point_conf wasn't created
        num_final_points = len(points_3d.reshape(-1, 3))
        if use_ba:
            if pred_confs is not None:
                point_conf_flat = pred_confs.reshape(-1, 1)
            else:
                point_conf_flat = np.ones((num_final_points, 1))
        else:
            point_conf_flat = filtered_conf_values.reshape(-1, 1)
    
    # Final shape validation
    num_final_points = len(points_3d.reshape(-1, 3))
    if point_conf_flat.shape[0] != num_final_points:
        print(f"FINAL WARNING: Confidence shape mismatch. Points: {num_final_points}, Confidence: {point_conf_flat.shape[0]}")
        if point_conf_flat.shape[0] > num_final_points:
            point_conf_flat = point_conf_flat[:num_final_points]
        else:
            padding = np.ones((num_final_points - point_conf_flat.shape[0], 1))
            point_conf_flat = np.vstack([point_conf_flat, padding])
    
    # Ensure (N, 1) format one final time
    point_conf_flat = point_conf_flat.reshape(-1, 1)
    
    # Normalize confidence for visualization
    if len(point_conf_flat) > 0:
        conf_normalized = (point_conf_flat - point_conf_flat.min()) / (point_conf_flat.max() - point_conf_flat.min() + 1e-8)
        
        # Create confidence-based colors
        conf_colors = np.zeros((num_final_points, 3))
        conf_colors[:, 0] = conf_normalized.reshape(-1) * 255  # Red for high confidence
        conf_colors[:, 2] = (1 - conf_normalized.reshape(-1)) * 255  # Blue for low confidence
        
        # Save additional visualization point clouds
        trimesh.PointCloud(points_3d.reshape(-1, 3), colors=points_rgb.reshape(-1, 3)).export(
            os.path.join(sparse_0_path, "points_rgb.ply"))
        trimesh.PointCloud(points_3d.reshape(-1, 3), colors=conf_colors.astype(np.uint8)).export(
            os.path.join(sparse_0_path, "points_confidence.ply"))
        
        print(f"Confidence visualization saved to {os.path.join(sparse_0_path, 'points_confidence.ply')}")
        print(f"Final point cloud shape: {points_3d.reshape(-1, 3).shape}")
        print(f"Final confidence shape: {point_conf_flat.shape}")
        
        # Save the properly shaped confidence array with multiple names for safety
        np.save(os.path.join(sparse_0_path, "point_confidence_final.npy"), point_conf_flat)
        np.save(os.path.join(sparse_0_path, "confidence_dsp.npy"), point_conf_flat)  # This is what training expects
        # np.save(os.path.join(sparse_0_path, "points3D_all.npy"), points_3d.reshape(-1, 3))
        # print(f"Saved raw 3D points to {sparse_0_path}/points3D_all.npy with shape {points_3d.reshape(-1, 3).shape}")
        print(f"Final confidence arrays saved with shape {point_conf_flat.shape}")
        print(f"Saved to: {sparse_0_path}/confidence.npy (for training)")
        print(f"Saved to: {sparse_0_path}/point_confidence_final.npy (for backup)")
    
    # Handle test images if needed
    if not infer_video:
        # ---------------- (2) Interpolate training pose to get initial testing pose ----------------
        n_train = len(train_img_files)  # 3
        n_test = len(test_img_files)    #12
        
        print(f'>> Interpolating poses for test images...')
        print(f'Training images: {n_train}, Test images: {n_test}')

        if n_train < n_test:
            # Convert extrinsic matrices to world-to-camera format for interpolation
            # VGGT extrinsic is already in world-to-camera format (camera from world)
            extrinsics_w2c = extrinsic  # Shape: [n_train, 4, 4]    Now is 3 3 4
            
            n_interp = (n_test // (n_train-1)) + 1
            all_inter_pose = []
            for i in range(n_train-1):
                # Extract 3x4 poses for interpolation
                pose_pair = extrinsics_w2c[i:i+2, :3, :]  # [2, 3, 4]
                tmp_inter_pose = generate_interpolated_path(poses=pose_pair, n_interp=n_interp)
                all_inter_pose.append(tmp_inter_pose)
            
            # Concatenate all interpolated poses
            all_inter_pose = np.concatenate(all_inter_pose, axis=0)
            # Add the last training pose
            all_inter_pose = np.concatenate([all_inter_pose, extrinsics_w2c[-1][:3, :].reshape(1, 3, 4)], axis=0)
            
            # Sample poses to match the number of test images
            indices = np.linspace(0, all_inter_pose.shape[0] - 1, n_test, dtype=int)
            sampled_poses = all_inter_pose[indices]
            sampled_poses = np.array(sampled_poses).reshape(-1, 3, 4)
            assert sampled_poses.shape[0] == n_test
            
            # Convert back to 4x4 matrices
            inter_pose_list = []
            for p in sampled_poses:
                tmp_view = np.eye(4)
                tmp_view[:3, :3] = p[:3, :3]
                tmp_view[:3, 3] = p[:3, 3]
                inter_pose_list.append(tmp_view)
            pose_test_init = np.stack(inter_pose_list, 0)
        else:
            # Don't meet this case
            # If we have enough training poses, just sample from them
            indices = np.linspace(0, extrinsic.shape[0] - 1, n_test, dtype=int)
            pose_test_init = extrinsic[indices]

        print(f'Generated {pose_test_init.shape[0]} test poses')
        print(f'Test pose shape: {pose_test_init.shape}')  # Should be [n_test, 4, 4]
        print(f'Extrinsic shape: {extrinsic.shape}')       # Should be [n_train, 4, 4]
        print(f'Intrinsic shape: {intrinsic.shape}')       # Should be [n_train, 3, 3]
        
        # Save test extrinsics
        save_extrinsic(sparse_1_path, pose_test_init, test_img_files, image_suffix)
        
        # Create test intrinsics (use the first training camera's intrinsics for all test images)
        # Extract focal length from the first intrinsic matrix
        first_intrinsic = intrinsic[0]  # [3, 3]
        focal_x = first_intrinsic[0, 0]
        focal_y = first_intrinsic[1, 1]
        # Use average focal length
        avg_focal = (focal_x + focal_y) / 2.0
        test_focals = np.repeat(avg_focal, n_test)
        
        # Get original image shapes for test images
        # Load a test image to get its original dimensions
        test_img_sample = PIL.Image.open(test_img_files[0])
        test_img_sample = exif_transpose(test_img_sample)
        test_img_org_shape = test_img_sample.size  # (width, height) after EXIF orientation
        try:
            org_sizes_test = []
            for f in test_img_files:
                im = exif_transpose(PIL.Image.open(f))
                org_sizes_test.append(im.size)
            save_intrinsics_from_mats(sparse_1_path, intrinsic_rescaled[[0]*len(test_img_files)], org_sizes_test)
        except Exception as _:
            test_imgs_shape = (len(test_img_files), vggt_fixed_resolution, vggt_fixed_resolution, 3)
            save_intrinsics(sparse_1_path, test_focals, test_img_org_shape, test_imgs_shape, save_focals=False)
        
        print(f'Test poses and intrinsics saved to: {str(sparse_1_path)}')
        print(f'Test focal length: {avg_focal:.2f}')
    # -----------------------------------------------------------------------------------------
    
    train_time = time() - start_time
    save_time(model_path, '[1] vggt_init_TrainTime', train_time)
    save_time(model_path, '[1] vggt_init_geo', train_time)
    
    print(f'[INFO] VGGT Reconstruction completed with {conf_source} confidence!')
    print(f'[INFO] Number of 3D points: {len(points_3d.reshape(-1, 3))}')
    print(f'[INFO] Sparse reconstruction saved to: {str(sparse_0_path)}')
    # np.save(os.path.join(sparse_0_path, "confidence_dsp.npy"), point_conf_flat)
    
    # Save the processed 512x512 images used by VGGT
    processed_img_dir = save_processed_images(
        sparse_0_path, 
        F.interpolate(images, size=(vggt_fixed_resolution, vggt_fixed_resolution), mode="bilinear", align_corners=False),
        image_path_list, 
        image_suffix,  # Use the already extracted image suffix
        prefix="vggt_518x518_"
    )

    # Add debug prints in your training initialization
    print(f"Point cloud shape: {points_3d.shape}")
    print(f"Expected: [N_points, 3], Got: {points_3d.shape}")

    # Remove POINTS2D empty lines from images.txt
    remove_points2d_data_keep_empty_lines(sparse_0_path)


def convert_colmap_bin_to_txt(sparse_0_path):
    """Convert COLMAP binary files to text format using pycolmap"""
    print("Converting COLMAP binary files to text format...")
    
    try:
        # Read the binary reconstruction
        reconstruction = pycolmap.Reconstruction(str(sparse_0_path))
        
        # Write as text format
        reconstruction.write_text(str(sparse_0_path))
        
        print(f"COLMAP text files saved:")
        print(f"  - {os.path.join(sparse_0_path, 'cameras.txt')}")
        print(f"  - {os.path.join(sparse_0_path, 'images.txt')}")
        print(f"  - {os.path.join(sparse_0_path, 'points3D.txt')}")
        
    except Exception as e:
        print(f"Error converting to text format: {e}")
        print("Keeping binary format only.")


def save_points3d_ply_with_normals(sparse_0_path, points_3d, colors, normals=None):
    """Save PLY file with proper format for dataset_readers.py"""
    points_3d = points_3d.reshape(-1, 3)
    colors = colors.reshape(-1, 3)
    
    # Generate dummy normals if not provided
    if normals is None:
        normals = np.zeros_like(points_3d)
        normals[:, 2] = 1.0  # Point upward
    
    # Create structured array with the expected field names
    vertex_data = np.array([
        (points_3d[i, 0], points_3d[i, 1], points_3d[i, 2],
         normals[i, 0], normals[i, 1], normals[i, 2],
         int(colors[i, 0]), int(colors[i, 1]), int(colors[i, 2]))
        for i in range(len(points_3d))
    ], dtype=[
        ('x', 'f4'), ('y', 'f4'), ('z', 'f4'),
        ('nx', 'f4'), ('ny', 'f4'), ('nz', 'f4'),
        ('red', 'u1'), ('green', 'u1'), ('blue', 'u1')
    ])
    
    # Create PLY element
    vertex_element = PlyElement.describe(vertex_data, 'vertex')
    
    # Write PLY file
    ply_path = os.path.join(sparse_0_path, "points3D.ply")
    PlyData([vertex_element]).write(ply_path)
    
    print(f"PLY file with normals saved to {ply_path}")
    return ply_path


def batch_np_matrix_to_pycolmap_with_names(
    points3d, extrinsics, intrinsics, tracks, image_size, image_names,
    masks=None, max_reproj_error=None, max_points3D_val=3000, 
    shared_camera=False, camera_type="SIMPLE_PINHOLE", 
    extra_params=None, min_inlier_per_frame=64, points_rgb=None,
):
    """Wrapper function that adds image names to the original function"""
    from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap
    
    # Call the original function
    reconstruction, valid_mask = batch_np_matrix_to_pycolmap(
        points3d, extrinsics, intrinsics, tracks, image_size,
        masks=masks, max_reproj_error=max_reproj_error, 
        max_points3D_val=max_points3D_val, shared_camera=shared_camera,
        camera_type=camera_type, extra_params=extra_params,
        min_inlier_per_frame=min_inlier_per_frame, points_rgb=points_rgb
    )
    
    if reconstruction is None:
        return None, valid_mask
    
    # Update image names with actual filenames
    for fidx, image_name in enumerate(image_names):
        if (fidx + 1) in reconstruction.images:
            reconstruction.images[fidx + 1].name = image_name
    
    return reconstruction, valid_mask


def batch_np_matrix_to_pycolmap_wo_track_with_names(
    points3d, points_xyf, points_rgb, extrinsics, intrinsics, 
    image_size, image_names, shared_camera=False, camera_type="SIMPLE_PINHOLE"
):
    """Wrapper function that adds image names to the original function"""
    from vggt.dependency.np_to_pycolmap import batch_np_matrix_to_pycolmap_wo_track
    
    # Call the original function
    reconstruction = batch_np_matrix_to_pycolmap_wo_track(
        points3d, points_xyf, points_rgb, extrinsics, intrinsics, 
        image_size, shared_camera=shared_camera, camera_type=camera_type
    )
    
    # Update image names with actual filenames
    for fidx, image_name in enumerate(image_names):
        if (fidx + 1) in reconstruction.images:
            reconstruction.images[fidx + 1].name = image_name
    
    return reconstruction


# def confidence_based_filtering(points_3d, conf_values, max_points=100000):
#     # Flatten all arrays
#     points_flat = points_3d.reshape(-1, 3)
#     conf_flat = conf_values.reshape(-1)
    
#     # Sort by confidence (highest confidence first)
#     sorted_indices = np.argsort(conf_flat)[::-1]  # From high to low
    
#     # Select top max_points highest confidence points
#     if len(sorted_indices) > max_points:
#         selected_indices = sorted_indices[:max_points]
#     else:
#         selected_indices = sorted_indices
        
#     return points_flat[selected_indices], conf_flat[selected_indices]


def remove_points2d_data_keep_empty_lines(sparse_0_path):
    """
    Remove POINTS2D coordinate data but keep empty lines to maintain COLMAP format
    
    Result format:
    IMAGE_ID QW QX QY QZ TX TY TZ CAMERA_ID NAME
    [empty line]
    IMAGE_ID QW QX QY QZ TX TY TZ CAMERA_ID NAME  
    [empty line]
    """
    import re
    
    images_txt_path = os.path.join(sparse_0_path, 'images.txt')
    
    if not os.path.exists(images_txt_path):
        print(f"Warning: {images_txt_path} not found, skipping processing")
        return
    
    print(f"Removing POINTS2D data from {images_txt_path} (keeping empty lines)")
    
    # Read the original file
    with open(images_txt_path, 'r') as f:
        lines = f.readlines()
    
    # Create backup first
    backup_path = os.path.join(sparse_0_path, 'images_with_points2d.txt')
    with open(backup_path, 'w') as f:
        f.writelines(lines)
    print(f"Original format backed up to {backup_path}")
    
    processed_lines = []
    processed_points2d_lines = 0
    kept_image_lines = 0
    
    # Pattern to identify IMAGE_ID lines
    image_id_pattern = re.compile(r'^\d+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+\d+\s+\S+\.\w+')
    
    for i, line in enumerate(lines):
        if line.startswith('#'):
            # Keep all header/comment lines
            processed_lines.append(line)
        elif image_id_pattern.match(line.strip()):
            # This is an IMAGE_ID line - keep it
            processed_lines.append(line)
            kept_image_lines += 1
            print(f"  Kept IMAGE_ID line {i+1}: {line.strip()[:80]}...")
        elif line.strip() == '':
            # Already empty line - keep it
            processed_lines.append(line)
        elif line.strip() != '' and not line.startswith('#'):
            # This is POINTS2D data - replace with empty line
            processed_lines.append('\n')  # Empty line with newline
            processed_points2d_lines += 1
            print(f"  Replaced POINTS2D line {i+1} with empty line")
    
    # Write the processed file back
    with open(images_txt_path, 'w') as f:
        f.writelines(processed_lines)
    
    print(f"Processing complete:")
    print(f"   - Kept {kept_image_lines} IMAGE_ID lines")
    print(f"   - Replaced {processed_points2d_lines} POINTS2D lines with empty lines")
    print(f"   - Format: 2 lines per image (IMAGE_ID + empty line)")


def keep_only_image_id_rows(sparse_0_path):
    """
    Post-process images.txt to keep only IMAGE_ID rows, 
    removing POINTS2D data rows and ensuring proper newlines
    """
    import re
    
    images_txt_path = os.path.join(sparse_0_path, 'images.txt')
    
    if not os.path.exists(images_txt_path):
        print(f"Warning: {images_txt_path} not found, skipping processing")
        return
    
    print(f"Removing POINTS2D data rows from {images_txt_path}")
    
    # Read the original file
    with open(images_txt_path, 'r') as f:
        lines = f.readlines()
    
    # Create backup first
    backup_path = os.path.join(sparse_0_path, 'images_with_points2d.txt')
    with open(backup_path, 'w') as f:
        f.writelines(lines)
    print(f"Original format backed up to {backup_path}")
    
    processed_lines = []
    removed_points2d_lines = 0
    kept_image_lines = 0
    
    # Pattern to identify IMAGE_ID lines: starts with number, has quaternion + translation + camera_id + filename
    image_id_pattern = re.compile(r'^\d+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+[\d\.\-e]+\s+\d+\s+\S+\.\w+')
    
    for i, line in enumerate(lines):
        if line.startswith('#'):
            # Keep all header/comment lines (they already have newlines)
            processed_lines.append(line)
        elif image_id_pattern.match(line.strip()):
            # This is an IMAGE_ID line - keep it and ensure it has a newline
            if not line.endswith('\n'):
                line = line.rstrip() + '\n'
            processed_lines.append(line)
            processed_lines.append('\n')
            kept_image_lines += 1
            print(f"  Kept IMAGE_ID line {i+1}: {line.strip()[:80]}...")
        elif line.strip() != '' and not line.startswith('#'):
            # This is likely a POINTS2D data line - skip it
            removed_points2d_lines += 1
            print(f"  Removed POINTS2D line {i+1}: {line.strip()[:80]}...")
        # Skip empty lines that are not in header
    
    # Write the processed file back
    with open(images_txt_path, 'w') as f:
        f.writelines(processed_lines)
    
    print(f"Processing complete:")
    print(f"   - Kept {kept_image_lines} IMAGE_ID lines")
    print(f"   - Removed {removed_points2d_lines} POINTS2D data lines")
    print(f"   - Each line properly terminated with newline")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='VGGT geometry initialization with comprehensive confidence')
    parser.add_argument('--source_path', '-s', type=str, required=True, help='Directory containing images')
    parser.add_argument('--model_path', '-m', type=str, required=True, help='Directory to save the results')
    parser.add_argument('--device', type=str, default='cuda', help='Device to use for inference')
    parser.add_argument('--min_conf_thr', type=float, default=5.0, help='Minimum confidence threshold')
    parser.add_argument('--llffhold', type=int, default=8, help='LLFF hold-out for test images')
    parser.add_argument('--n_views', type=int, default=3, help='Number of views to process')
    parser.add_argument('--use_ba', action="store_true", help='Use Bundle Adjustment')
    parser.add_argument('--infer_video', action="store_true", help='Video mode (only populate sparse/0)')
    
    # BA-specific parameters
    parser.add_argument('--max_reproj_error', type=float, default=8.0, help='Maximum reprojection error for BA')
    parser.add_argument('--shared_camera', action="store_true", help='Use shared camera for all images')
    parser.add_argument('--camera_type', type=str, default="SIMPLE_PINHOLE", help='Camera type for reconstruction')
    parser.add_argument('--vis_thresh', type=float, default=0.2, help='Visibility threshold for tracks')
    parser.add_argument('--query_frame_num', type=int, default=8, help='Number of frames to query for tracking')
    parser.add_argument('--max_query_pts', type=int, default=4096, help='Maximum number of query points')
    parser.add_argument('--fine_tracking', action="store_true", default=True, help='Use fine tracking')
    parser.add_argument('--image_size', type=int, default=518, help='Size to resize images (same as MASt3R)')

    args = parser.parse_args()
    
    vggt_kwargs = {
        'max_reproj_error': args.max_reproj_error,
        'shared_camera': args.shared_camera,
        'camera_type': args.camera_type,
        'vis_thresh': args.vis_thresh,
        'query_frame_num': args.query_frame_num,
        'max_query_pts': args.max_query_pts,
        'fine_tracking': args.fine_tracking,
    }
    
    main(args.source_path, args.model_path, args.device, args.min_conf_thr, 
         args.llffhold, args.n_views, args.image_size, args.use_ba, args.infer_video, **vggt_kwargs)